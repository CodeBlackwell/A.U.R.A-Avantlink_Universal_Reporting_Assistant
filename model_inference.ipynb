{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install optimum torch torchvision torchaudio\n",
    "# %pip install --upgrade notebook ipywidgets\n",
    "# python 3.8 \n",
    "# cuda 11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/juke/text-generation-webui/models/TheBloke_deepseek-coder-6.7B-base-AWQ',\n",
      " '/home/juke/text-generation-webui/models/TheBloke_Mistral-7B-Instruct-v0.1-GPTQ',\n",
      " '/home/juke/text-generation-webui/models/WizardLM_WizardCoder-Python-13B-V1.0',\n",
      " '/home/juke/text-generation-webui/models/TheBloke_WizardCoder-15B-1.0-GPTQ',\n",
      " '/home/juke/text-generation-webui/models/TheBloke_Starcoderplus-Guanaco-GPT4-15B-V1.0-GPTQ',\n",
      " '/home/juke/text-generation-webui/models/WizardLM_WizardCoder-15B-V1.0',\n",
      " '/home/juke/text-generation-webui/models/TheBloke_Wizard-Vicuna-7B-Uncensored-GPTQ',\n",
      " '/home/juke/text-generation-webui/models/mistralai_Mistral-7B-v0.1',\n",
      " '/home/juke/text-generation-webui/models/TheBloke_Llama-2-70B-chat-GPTQ',\n",
      " '/home/juke/text-generation-webui/models/TheBloke_deepseek-coder-6.7B-instruct-AWQ',\n",
      " '/home/juke/text-generation-webui/models/google_flan-t5-xl',\n",
      " '/home/juke/text-generation-webui/models/mistralai_Mistral-7B-Instruct-v0.1',\n",
      " '/home/juke/text-generation-webui/models/TheBloke_WizardCoder-Guanaco-15B-V1.1-GPTQ',\n",
      " '/home/juke/text-generation-webui/models/codellama_CodeLlama-13b-Instruct-hf']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model_path = \"/home/juke/text-generation-webui/models/\"\n",
    "all_files = []\n",
    "\n",
    "# List all entries in the directory\n",
    "for entry in os.listdir(base_model_path):\n",
    "    full_path = os.path.join(base_model_path, entry)\n",
    "\n",
    "    # Check if it's a directory\n",
    "    if not os.path.isfile(full_path):\n",
    "        all_files.append(full_path)\n",
    "\n",
    "pprint(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/juke/Scripts/operation_sk/model_inference.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juke/Scripts/operation_sk/model_inference.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdeepseek-ai/deepseek-coder-6.7b-instruct\u001b[39m\u001b[39m\"\u001b[39m, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juke/Scripts/operation_sk/model_inference.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mdeepseek-ai/deepseek-coder-6.7b-instruct\u001b[39;49m\u001b[39m\"\u001b[39;49m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juke/Scripts/operation_sk/model_inference.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m messages\u001b[39m=\u001b[39m[\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juke/Scripts/operation_sk/model_inference.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     { \u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mwrite a quick sort algorithm in python.\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juke/Scripts/operation_sk/model_inference.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juke/Scripts/operation_sk/model_inference.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mapply_chat_template(messages, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:3236\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[39m=\u001b[39mtorch_dtype, device_map\u001b[39m=\u001b[39mdevice_map)\n\u001b[1;32m   3235\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3236\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   3238\u001b[0m \u001b[39m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:961\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    960\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m--> 961\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlamaModel(config)\n\u001b[1;32m    962\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    963\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:823\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    822\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[0;32m--> 823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    824\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    826\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:823\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    822\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[0;32m--> 823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    824\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    826\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:634\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m (\n\u001b[1;32m    630\u001b[0m     LlamaAttention(config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39m_flash_attn_2_enabled\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    632\u001b[0m     \u001b[39melse\u001b[39;00m LlamaFlashAttention2(config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m    633\u001b[0m )\n\u001b[0;32m--> 634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m LlamaMLP(config)\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    636\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:236\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mintermediate_size\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_size, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn \u001b[39m=\u001b[39m ACT2FN[config\u001b[39m.\u001b[39mhidden_act]\n",
      "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/init.py:419\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    417\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True).cuda()\n",
    "# messages=[\n",
    "#     { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "# # 32021 is the id of <|EOT|> token\n",
    "# outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "# print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
